**Evaluation: Assessing Text Performance with Precision ğŸ“ŠğŸ’¡**
====================================================================

Evaluation is a comprehensive tool designed to measure the performance of text-based inputs, enabling data-driven optimization and improvement ğŸ“ˆ. 

**Text Evaluation 101 ğŸ“š**

Using robust framework for assessing reference and candidate texts across various metricsğŸ“Š, ensure that the text outputs are high-quality and meet specific requirements and standardsğŸ“.

| **Evaluation** | **Description** | **Links** |
| -------------- | --------------- | --------- |
| **Evaluating Prompts with Prompttools ğŸ¤–** | Compare, visualize & evaluate **embedding functions** (incl. OpenAI) across metrics like latency & custom evaluation ğŸ“ˆğŸ“Š | [![Github](../../assets/github.svg)][prompttools_github] <br>[![Open In Collab](../../assets/colab.svg)][prompttools_colab] |
| **Evaluating RAG with RAGAs and GPT-4o ğŸ“Š** | Evaluate **RAG pipelines** with cutting-edge metrics and tools, integrate with CI/CD for continuous performance checks, and generate responses with GPT-4o ğŸ¤–ğŸ“ˆ | [![Github](../../assets/github.svg)][RAGAs_github] <br>[![Open In Collab](../../assets/colab.svg)][RAGAs_colab] |



[prompttools_github]: https://github.com/lancedb/vectordb-recipes/blob/main/examples/prompttools-eval-prompts
[prompttools_colab]: https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/prompttools-eval-prompts/main.ipynb

[RAGAs_github]: https://github.com/lancedb/vectordb-recipes/blob/main/examples/Evaluating_RAG_with_RAGAs
[RAGAs_colab]: https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Evaluating_RAG_with_RAGAs/Evaluating_RAG_with_RAGAs.ipynb
